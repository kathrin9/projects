# -*- coding: utf-8 -*-
"""housing_ergasia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jd1ddgeW-sB7i6fWAjlJGNz0kDSgW1cK
"""

#import libraries
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#suspress warnings
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/housing.csv')
df.head()

#I want to see the number of different values of each column
df.nunique()

#print the number of rows/columns of the dataset
df.shape

#print columns name
df.columns

#print basic info of each column
df.info()

#I can see which columns have nan values
df.isna().sum()

#check for duplicated rows
df.duplicated().sum()

#I calculate the percentage of NaNs for each column, display the columns with over 40% NaNs and remove them from the dataset
nan_percent = df.isna().mean()
columns_to_drop = nan_percent[nan_percent > 0.4]
print("Î£Ï„Î®Î»ÎµÏ‚ Î¼Îµ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ 40% NaN:")
print(columns_to_drop.sort_values(ascending=False))
df= df.drop(columns=columns_to_drop.index)

print("\n ÎÎ­Î¿ ÏƒÏ‡Î®Î¼Î± Ï„Î¿Ï… dataset:", df.shape)

# Remove columns that are not needed
df.drop(columns=[  'Id', 'LotFrontage', 'LotShape' , 'Street' , 'LotConfig', 'YearBuilt' , 'RoofStyle', 'RoofMatl', 'ExterCond', 'MasVnrArea' ,
                'BsmtCond' , 'BsmtFinType1' , 'BsmtFinType2' , 'BsmtFinSF1' , 'BsmtFinSF2' , 'BsmtUnfSF' , 'HeatingQC' , 'Electrical' ,
                'LowQualFinSF' , 'BsmtFullBath' , 'BsmtHalfBath' , 'FullBath' , 'HalfBath' , 'BedroomAbvGr' , 'KitchenAbvGr' , 'KitchenQual' ,
                'TotRmsAbvGrd' ,  'GarageYrBlt', 'GarageCars' , 'GarageFinish', 'GarageCond', 'GarageQual',  'PavedDrive' ,
                'MoSold', 'YrSold', 'SaleType',  'PoolArea' , 'WoodDeckSF' , 'OpenPorchSF' , 'EnclosedPorch' , '3SsnPorch' , 'ScreenPorch' , 'MiscVal' ], inplace=True)

df.head()

df.info()

#Replace nan values of the columns BsmtQual, BsmtExposure, GarageType with the mode of each column
df['BsmtQual'] = df['BsmtQual'].fillna(df['BsmtQual'].mode().iloc[0])
df['BsmtExposure'] = df['BsmtExposure'].fillna(df['BsmtExposure'].mode().iloc[0])
df['GarageType'] = df['GarageType'].fillna(df['GarageType'].mode().iloc[0])

#Convert categorical variables into indicator variables
df = pd.get_dummies(df, drop_first=True)
df.head()
df.shape

#Visualizing  Outliers Using Box Plot
#create a box plot
import plotly.express as px
fig = px.box(df, y='SalePrice')

fig.show()

def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]


df = remove_outliers_iqr(df, 'SalePrice')

#create a box plot
import plotly.express as px
fig = px.box(df, y='SalePrice')

fig.show()

df.describe()

# check for correlation
df.corr()

#  Get columns with correlation > 0.4 with SalePrice and Plot  a heatmap
# Select numeric columns
numeric_df = df.select_dtypes(include=["int64", "float64"])

# Compute correlation matrix
correlation_matrix = numeric_df.corr()

# Get columns with correlation > 0.4 with SalePrice
strong_corr = correlation_matrix["SalePrice"].abs()
high_corr_cols = strong_corr[strong_corr > 0.4].index

# Filter the correlation matrix to only those columns
corr_subset = correlation_matrix.loc[high_corr_cols, high_corr_cols]

# Plot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_subset, annot=True, cmap="Blues", fmt=".1f", square=True)
plt.title("Heatmap of Features Strongly Correlated with SalePrice")
plt.tight_layout()
plt.show()

# Optional: print the list of those columns
print("ðŸ“Œ Columns strongly correlated with SalePrice:")
print(high_corr_cols.tolist())

# Train the model
# Define target variable and features
X = df.drop(columns=['SalePrice'])
y = df['SalePrice']

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

# Train a multiple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

print("Training RÂ² Score:", train_score)
print("Testing RÂ² Score:", test_score)

# Evaluate the model

mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("RMSE:", rmse)
print("RÂ² Score:", r2)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot(y_test, y_test, color='red')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Multiple Linear Regression: Actual vs Predicted Prices')
plt.show()

#apply lasso and Ridge to simplify our model
from sklearn import linear_model
lasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)
lasso_reg.fit(X_train, y_train)
lasso_reg.score(X_test, y_test)
lasso_reg.score(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score
lasso_predictions = lasso_reg.predict(X_test)
lasso_mse2 = mean_squared_error( y_test, lasso_predictions)
lasso_r22 = r2_score( y_test, lasso_predictions)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(y_test, lasso_predictions, color='blue')
plt.plot(y_test, y_test, color='red')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Lasso Linear Regression: Actual vs Predicted Prices')
plt.show()
print(f"Mean Squared Error: {lasso_mse2}, R^2 Score: {lasso_r22}")

from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=50, max_iter=100, tol=0.1)
ridge_reg.fit(X_train, y_train)
ridge_reg.score(X_test, y_test)
ridge_reg.score(X_train, y_train)

ridge_predictions = ridge_reg.predict(X_test)
ridge_mse2 = mean_squared_error(y_test, ridge_predictions)
ridge_r22 = r2_score(y_test, ridge_predictions)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(y_test, ridge_predictions, color='blue')
plt.plot(y_test, y_test, color='red')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Ridge Linear Regression: Actual vs Predicted Prices')
plt.show()
print(f"Mean Squared Error: {ridge_mse2}, R^2 Score: {ridge_r22}")

# Test our model
# Select 5 random rows using pandas' sample function
df_sample = X_test.sample(n=5, random_state=15)

# Get corresponding actual prices
y_sample_actual = y_test.loc[df_sample.index]

# Predict prices for the selected samples
y_sample_pred = model.predict(df_sample)

# Create a DataFrame to compare actual vs predicted prices
df_comparison_alt = pd.DataFrame({
    'Actual Price': y_sample_actual.values,
    'Predicted Price': y_sample_pred
}, index=y_sample_actual.index)

# Display results
df_comparison_alt

#After exploring the file and studying which columns I didn't need to predict the price of the house, I removed them.
#I also removed the columns that had nan values â€‹â€‹over 40% of the dataset. I replaced all the nan values â€‹â€‹they had sent, checked for duplicate rows and outliers.
#Then I looked at the correlation of the columns and applied Multiple Linear Regression.

# The multiple linear regression achieved an RÂ² Score of approximately 86% on testing dataset and 88% on training dataset.
#The model appears to be performing well both on the training and testing datasets, with a slight reduction in predictive power when generalizing to new data.
#This slight drop may indicate some overfitting, but it's not severe. The model still performs well on unseen data